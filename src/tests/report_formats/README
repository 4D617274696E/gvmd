Explanation of test mechanism
-----------------------------

The script test_report_format takes two arguments.

test_report_format <format> <input>

with
 - format: one of the directory names in
           openvas-manager/src/report_formats (e.g. NBE or HTML)
 - input:  one of the report xml files in
           openvas-manager/src/tests/report_formats/input
           __without file extension___

It generates the corresponding report in
  openvas-manager/src/tests/report_formats/generated_output/
and compares it against the desired output in
  openvas-manager/src/tests/report_formats/expected_output/
If there is a difference, it is saved in
  openvas-manager/src/tests/report_formats/diffs


How to add a test
-----------------

1) Place the "source" xml report in
   openvas-manager/src/tests/report_formats/input (or reuse one)
   Example: "empty_report.xml"
2) Get hold of the desired output and put it in
   openvas-manager/src/tests/report_formats/expected_output .
   Follow the naming convention.
   Example: "empty_report.pdf"
3) Add the test to CMakeLists.txt, like
   add_test (nbe_report_base PDF empty_report)


How to execute tests
--------------------

`cmake . && make test`
If the test failed, a diff is placed in the diffs/- subdirectory.


Caveats, todos for volunteers
-----------------------------

- Implicit but straigt-forward naming conventions
- Need to sweep/backup diffs/- subdirectory yourself
- test_report_format script is quick&dirty, not documented
- Naming could still be better
- Relatively inflexible (could be implemented as targets etc with cmake)
